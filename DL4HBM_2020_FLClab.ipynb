{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL4HBM-2020_FLClab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-B0zVJVTpEQ",
        "colab_type": "text"
      },
      "source": [
        "# Weakly Supervised Learning \n",
        "\n",
        "This notebook will allow a user to experiment with weakly supervised learning. It will provide the user some guidance about the challenges that can be faced in the weakly supervised field.\n",
        "\n",
        "The notebook is split into two parts. \n",
        "\n",
        "A first part introduces the how to use weakly supervised. At the end of this part a user should be able to qualitatively visualize the performance of the trained networks, but also obtain quantitative measurement of the performance.\n",
        "\n",
        "A second part will dig into the performance assessment of weakly supervised learning. The user will learn how to show the increase in performance of the trained network compared to the labels it was provided with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG34VgLv-HQG",
        "colab_type": "text"
      },
      "source": [
        "# Part 1 : Introduction to Weakly Supervised Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fe0_TLJdm1_3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy\n",
        "import glob, os  \n",
        "import torch\n",
        "import warnings\n",
        "import sys\n",
        "import pickle\n",
        "import skimage\n",
        "\n",
        "from matplotlib import pyplot, patches\n",
        "from sklearn.metrics import (confusion_matrix, f1_score, precision_score, recall_score,\n",
        "                             precision_recall_curve)\n",
        "from skimage import filters, io, draw\n",
        "from scipy.spatial import distance\n",
        "from collections import defaultdict\n",
        "\n",
        "CUDA = torch.cuda.is_available()\n",
        "if not CUDA: \n",
        "  print(\"Cuda is not available in the current notebook.\")\n",
        "  print(\"You can change this setting in Edit/Notebook settings.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpYbxKq7k8yu",
        "colab_type": "text"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhDHl8bsp8tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load data from public GitHub repo\n",
        "# Won't work for now since the repo is private, but once it is public it will\n",
        "# For now use the next cell to clone the private repo\n",
        "\n",
        "!git clone https://github.com/FLClab/DL4HBM-2020.git\n",
        "os.chdir(\"DL4HBM-2020\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHIxkwFGVIHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Allows to clone a private repository. I deleted user and password to avoid \n",
        "# leaking our identification. \n",
        "from getpass import getpass\n",
        "\n",
        "# Changes directory to the main folder\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "user = getpass('GitHub user')\n",
        "password = getpass('GitHub password')\n",
        "os.environ['GITHUB_AUTH'] = user + ':' + password\n",
        "if os.path.isdir(\"/content/DL4HBM-2020\"):\n",
        "  os.chdir(\"DL4HBM-2020\")\n",
        "  !git pull \n",
        "else:\n",
        "  !git clone https://$GITHUB_AUTH@github.com/FLClab/DL4HBM-2020.git\n",
        "  # Changes the directory to work on the repo\n",
        "  os.chdir(\"DL4HBM-2020\")\n",
        "\n",
        "# Removes traces of connection\n",
        "del user, password, os.environ[\"GITHUB_AUTH\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxNwesVRlDbv",
        "colab_type": "text"
      },
      "source": [
        "## Load user-defined utils functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USdANh46e7CF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.auto import tqdm\n",
        "\n",
        "class MetricCalculator:\n",
        "  \"\"\"\n",
        "  Implements a MetricCalculator class to facilite the calculation of metric \n",
        "  between the targets and the predictions\n",
        "  \"\"\"\n",
        "  def __init__(self, targets, predictions, foregrounds=None):\n",
        "    \"\"\"\n",
        "    Instantiates the `MetricCalculator` class\n",
        "\n",
        "    :param targets: A `numpy.ndarray` of the targets \n",
        "    :param targets: A `numpy.ndarray` of the predictions \n",
        "    :param targets: A `numpy.ndarray` of the foregrounds\n",
        "    \"\"\"\n",
        "    # Assign member variables \n",
        "    self.targets = targets\n",
        "    self.predictions = predictions\n",
        "    if isinstance(foregrounds, type(None)):\n",
        "      self.foregrounds = [None] * len(self.targets)\n",
        "    else:\n",
        "      self.foregrounds = foregrounds\n",
        "\n",
        "  def get(self, metric_names, **kwargs):\n",
        "    \"\"\"\n",
        "    Implements a get method to get the metric score between the targets\n",
        "    and the predictions\n",
        "\n",
        "    :param metrics: A `list` of the metrics to compute\n",
        "\n",
        "    :returns : A `list` of scores \n",
        "    \"\"\"\n",
        "    try:\n",
        "      scorers = [getattr(self, f\"_{metric_name}\") for metric_name in metric_names]\n",
        "    except AttributeError:\n",
        "      warnings.warn(f\"The chosen method `{metric_name}` does not exist.\\nExiting...\", category=UserWarning)\n",
        "      return\n",
        "    all_scores = []\n",
        "    for t, p, f in zip(tqdm(self.targets, leave=False), self.predictions, self.foregrounds):\n",
        "      p = numpy.argmax(p, axis=0) if p.shape[0] > 1 else p.squeeze()\n",
        "      if not isinstance(f, type(None)):\n",
        "        t = t[f.astype(bool)]\n",
        "        p = p[f.astype(bool)]\n",
        "      if (not numpy.any(t)) and ((not numpy.any(p)) or (\"precision_recall_curve\" in metric_names)):\n",
        "        continue  \n",
        "      scores = [scorer(t, p, **kwargs) for scorer in scorers]\n",
        "      all_scores.append(scores)\n",
        "    if \"precision_recall_curve\" in metric_names:\n",
        "      return all_scores\n",
        "    return numpy.array(all_scores).T\n",
        "\n",
        "  def _confusion_matrix(self, target, prediction, normalized=False):\n",
        "    \"\"\"\n",
        "    Computes the confusion matrix between the target and the prediction\n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : A 2x2 `numpy.ndarray` of the confusion matrix\n",
        "    \"\"\"\n",
        "    if target.ndim > 1:\n",
        "      target, prediction = target.ravel(), prediction.ravel()\n",
        "    truth, prediction = target.astype(bool), prediction.astype(bool)\n",
        "    cm = confusion_matrix(target, prediction)\n",
        "    if normalized:\n",
        "      cm = cm / (cm.sum(axis=1)[:, numpy.newaxis] + 1e-12)\n",
        "    return cm\n",
        "\n",
        "  def _iou(self, target, prediction):\n",
        "    \"\"\"\n",
        "    Computes the intersection over union\n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : Intersection over union\n",
        "    \"\"\"\n",
        "    target, prediction = target.astype(bool), prediction.astype(bool)\n",
        "    intersection = (target * prediction).sum()\n",
        "    union = (target + prediction).sum() + 1e-12\n",
        "    return intersection / union\n",
        "\n",
        "  def _dice(self, target, prediction):\n",
        "    \"\"\"\n",
        "    Computes the dice similarity coefficient\n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : Dice similarity coefficient\n",
        "    \"\"\"\n",
        "    target, prediction = target.astype(bool), prediction.astype(bool)\n",
        "    intersection = (target * prediction).sum()\n",
        "    return 2 * intersection / (target.sum() + prediction.sum() + 1e-12)\n",
        "\n",
        "  def _f1_score(self, target, prediction):\n",
        "    \"\"\"\n",
        "    Computes the f1 score between the target and the prediction\n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : F1-score\n",
        "    \"\"\"\n",
        "    if target.ndim > 1:\n",
        "      target, prediction = target.ravel(), prediction.ravel()\n",
        "    target, prediction = target.astype(bool), prediction.astype(bool)\n",
        "    return self._dice(target, prediction)\n",
        "\n",
        "  def _precision(self, target, prediction):\n",
        "    \"\"\"\n",
        "    Computes the precision between the target and the prediction\n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : Precision\n",
        "    \"\"\"\n",
        "    if target.ndim > 1:\n",
        "      target, prediction = target.ravel(), prediction.ravel()    \n",
        "    target, prediction = target.astype(bool), prediction.astype(bool)\n",
        "    return precision_score(target, prediction, zero_division=0)\n",
        "\n",
        "  def _recall(self, target, prediction):\n",
        "    \"\"\"\n",
        "    Computes the recall between the target and the prediction\n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : Recall\n",
        "    \"\"\"\n",
        "    if target.ndim > 1:\n",
        "      target, prediction = target.ravel(), prediction.ravel()\n",
        "    target, prediction = target.astype(bool), prediction.astype(bool)\n",
        "    return recall_score(target, prediction, zero_division=0) \n",
        "\n",
        "  def _accuracy(self, target, prediction):\n",
        "    \"\"\"\n",
        "    Computes the accuracy between the target and the prediction\n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : accuracy\n",
        "    \"\"\"\n",
        "    if target.ndim > 1:\n",
        "      target, prediction = target.ravel(), prediction.ravel()\n",
        "    target, prediction = target.astype(bool), prediction.astype(bool)\n",
        "    return (target == prediction).sum() / len(target)\n",
        "\n",
        "  def _precision_recall_curve(self, target, prediction):\n",
        "    \"\"\"\n",
        "    Computes the precision recall curve between the target and the prediction \n",
        "\n",
        "    :param target: A `numpy.ndarray` with shape [H, W] of the target\n",
        "    :param prediction: A `numpy.ndarray` with shape [H, W] of the prediction\n",
        "\n",
        "    :returns : A `numpy.ndarray` of precision\n",
        "    :returns : A `numpy.ndarray` of recall\n",
        "    :returns : A `numpy.ndarray` of thresholds\n",
        "    \"\"\"   \n",
        "    if target.ndim > 1:\n",
        "      target, prediction = target.ravel(), prediction.ravel()\n",
        "    target = target.astype(bool)\n",
        "    return precision_recall_curve(target, prediction)\n",
        "\n",
        "def sigmoid(x):\n",
        "  \"\"\"\n",
        "  Calculates the sigmoid transform of the input array \n",
        "\n",
        "  :param x: A `numpy.ndarray`\n",
        "\n",
        "  :returns : A `numpy.ndarray` of the transformed array\n",
        "  \"\"\"\n",
        "  x = numpy.clip(x, -10, 10) # Avoids overflow\n",
        "  return 1 / (1 + numpy.exp(-x))\n",
        "\n",
        "def plot_learning_curves(model_path, title=None):\n",
        "  \"\"\"\n",
        "  Plots the learning curves of the loaded model\n",
        "\n",
        "  :param model_path: The path of the loaded model \n",
        "  \"\"\"\n",
        "  stats = pickle.load(open(os.path.join(model_path, \"statsCkpt_490.pkl\"), \"rb\"))\n",
        "  fig, ax = pyplot.subplots()\n",
        "  for condition in [\"train\", \"test\"]:\n",
        "    mean, std = map(numpy.array, (stats[f\"{condition}Mean\"], stats[f\"{condition}Std\"]))\n",
        "    epochs = numpy.arange(len(mean))\n",
        "    ax.plot(epochs, mean, label=\"validation\" if condition == \"test\" else condition)\n",
        "    if condition == \"test\":\n",
        "      ax.axvline(x=numpy.argmin(mean), color=\"black\", linestyle=\"dashed\")\n",
        "    ax.fill_between(epochs, mean - std, mean + std)\n",
        "  ax.legend()\n",
        "  ax.set(\n",
        "      xlabel=\"Epochs\", ylabel=\"Cross Entropy Loss\",\n",
        "      title=title\n",
        "  )\n",
        "  pyplot.show()\n",
        "\n",
        "def show_random(*args, samples=5):\n",
        "  \"\"\"\n",
        "  Randomly sample from the given the input arrays \n",
        "\n",
        "  :param *args: `numpy.ndarray` with shape [B, C, H, W]\n",
        "  :param samples: The number of samples to sample from each arrays  \n",
        "  \"\"\"\n",
        "  samples = numpy.random.choice(len(args[0]), size=min(samples, len(args[0])), replace=False)\n",
        "  for sample in samples:\n",
        "    fig, axes = pyplot.subplots(1, len(args), figsize=(8, 3))\n",
        "    for ax, ary in zip(axes.ravel(), args):\n",
        "      ary = numpy.squeeze(ary) # removes the empty channel from the image\n",
        "      if ary.ndim == 3:\n",
        "        ax.imshow(ary[sample], vmax=0.3 * ary[sample].max(), cmap=\"gray\")\n",
        "      else:\n",
        "        ax.imshow(numpy.argmax(ary[sample], axis=0), cmap=\"gray\")\n",
        "        # ax.imshow(sigmoid(ary[sample, 1]), vmin=0, vmax=1, cmap=\"gray\")\n",
        "    \n",
        "    for ax in axes:\n",
        "      ax.axis(\"off\")\n",
        "  pyplot.show()\n",
        "    \n",
        "def plot_scores(data, show_points=True, avail_metrics=[\"iou\"], ylim=(0,1),\n",
        "                show_lines=False, rotation=None):\n",
        "  \"\"\"\n",
        "  Plots the scores from the given data \n",
        "\n",
        "  :param data: A `dict` of scores with the following architecture \n",
        "               {condition : {metric : []}}\n",
        "\n",
        "  :returns : A `matplotlib.Figure` instance \n",
        "  :returns : A `matplotlib.Axes` instance \n",
        "  \"\"\"\n",
        "  # Creates the matplotlib figure and define constants\n",
        "  fig, ax = pyplot.subplots()\n",
        "  num_metrics = len(avail_metrics)\n",
        "  width = 1 / (num_metrics + 1)\n",
        "  cmap = pyplot.cm.get_cmap(\"tab10\")\n",
        "\n",
        "  # Plots the data \n",
        "  conditions, metrics = [], []\n",
        "  for i, (condition, scores) in enumerate(data.items()):\n",
        "    conditions.append(condition)\n",
        "    j = 0\n",
        "    for (metric, values) in scores.items():\n",
        "      if metric not in avail_metrics: continue\n",
        "      if i == 0: metrics.append(metric)\n",
        "      # ax.bar(i + j * width, numpy.mean(values), yerr=numpy.std(values), \n",
        "      #        align=\"edge\", width=width, color=cmap(j), alpha=0.8)\n",
        "      ax.bar(i + j * width, numpy.median(values), yerr=numpy.diff(numpy.quantile(values, [0.5, 0.75])), \n",
        "             align=\"edge\", width=width, color=cmap(j), alpha=0.8)\n",
        "\n",
        "      # Add the scatter points to the graph\n",
        "      if show_points:\n",
        "        xs = numpy.random.normal(loc=i + j * width + width / 2, scale=width/25, size=len(values))\n",
        "        ax.scatter(xs, values, color=\"black\", alpha=0.3, s=10)\n",
        "\n",
        "      j += 1\n",
        "    \n",
        "  if show_lines:\n",
        "    for metric in avail_metrics:\n",
        "      delta = metrics.index(metric)\n",
        "      for points in zip(*[data[condition][metric] for condition in conditions]):\n",
        "        ax.plot(numpy.arange(len(points)) + delta * width + width / 2, points,\n",
        "                color=\"black\", alpha=0.3)\n",
        "  \n",
        "  # Sets the axes \n",
        "  ax.set(\n",
        "      ylim = ylim,\n",
        "      xticks = numpy.arange(len(data)) + width * num_metrics / 2,\n",
        "      xticklabels = conditions,\n",
        "      ylabel = \"Scores\"\n",
        "  )\n",
        "  if not isinstance(rotation, type(None)):\n",
        "    ax.set_xticklabels(conditions, rotation=rotation)\n",
        "\n",
        "  # Sets the legend of the graph\n",
        "  ax.legend(handles=[patches.Patch(color=cmap(i), label=metric) for i, metric in enumerate(metrics)])\n",
        "\n",
        "  return fig, ax\n",
        "\n",
        "def plot_cumfreq(data, avail_metrics=[\"iou\"]):\n",
        "  \"\"\"\n",
        "  Plots the scores from the given data \n",
        "\n",
        "  :param data: A `dict` of scores with the following architecture \n",
        "               {condition : {metric : []}}\n",
        "\n",
        "  :returns : A `matplotlib.Figure` instance \n",
        "  :returns : A `matplotlib.Axes` instance \n",
        "  \"\"\"\n",
        "  # Creates the matplotlib figure and define constants\n",
        "  fig, ax = pyplot.subplots()\n",
        "  cmap = pyplot.cm.get_cmap(\"tab10\")\n",
        "\n",
        "  # Plots the data \n",
        "  conditions, metrics = [], []\n",
        "  linestyles = [\"-\", \"--\", \"-.\"]\n",
        "  for i, (condition, scores) in enumerate(data.items()):\n",
        "    conditions.append(condition)\n",
        "\n",
        "    j = 0\n",
        "    for (metric, values) in scores.items():\n",
        "      if metric not in avail_metrics: continue\n",
        "      if i == 0: metrics.append(metric)\n",
        "\n",
        "      hist, bins = numpy.histogram(values)\n",
        "      cumsum = numpy.cumsum(hist) / hist.sum()\n",
        "      ax.plot(bins[:-1], cumsum, color=cmap(j), linestyle=linestyles[i])\n",
        "\n",
        "      j += 1\n",
        "\n",
        "  # Sets the axes \n",
        "  ax.set(\n",
        "      ylim = (0,1),\n",
        "      ylabel = \"Cumulative frequency\"\n",
        "  )\n",
        "\n",
        "  # Sets the legend of the graph\n",
        "  ax.legend(handles=[patches.Patch(color=cmap(i), label=metric) for i, metric in enumerate(metrics)])\n",
        "\n",
        "  return fig, ax\n",
        "\n",
        "def plot_cm(cm):\n",
        "  \"\"\"\n",
        "  Plots a confusion matrix \n",
        "\n",
        "  :param cm: A `numpy.ndarray` of the confusion matrix \n",
        "\n",
        "  :returns : A `matplotlib.Figure` instance \n",
        "  :returns : A `matplotlib.Axes` instance \n",
        "  \"\"\"\n",
        "  fig, ax = pyplot.subplots()\n",
        "  ax.imshow(cm, cmap=\"Blues\")\n",
        "  for j in range(cm.shape[0]):\n",
        "    for i in range(cm.shape[1]):\n",
        "      ax.text(j, i, \"{:0.4f}\".format(cm[j, i]), horizontalalignment=\"center\", verticalalignment=\"center\")\n",
        "  ax.set(\n",
        "      xticks=numpy.arange(cm.shape[1]), xticklabels=[\"Background\", \"Structure\"],\n",
        "      yticks=numpy.arange(cm.shape[1]), yticklabels=[\"Background\", \"Structure\"],\n",
        "  )\n",
        "  return fig, ax\n",
        "\n",
        "def show_average_cm(data):\n",
        "  \"\"\"\n",
        "  Plots the average confusion matrix from data \n",
        "\n",
        "  :param data: A `dict` of scores with the following architecture \n",
        "               {condition : {metric : []}}\n",
        "  \n",
        "  returns: A `list` of (fig, ax) tuple\n",
        "  \"\"\"\n",
        "  conditions, metrics = [], []\n",
        "  output = []\n",
        "  for i, (condition, scores) in enumerate(data.items()):\n",
        "    conditions.append(condition)\n",
        "    for j, (metric, values) in enumerate(scores.items()):\n",
        "      if i == 0: metrics.append(metric)\n",
        "      if metric != \"confusion_matrix\": continue\n",
        "      cms = numpy.array(values)\n",
        "      cm = numpy.sum(cms, axis=0)\n",
        "      cm = cm / (cm.sum(axis=0, keepdims=True) + 1e-12)\n",
        "      fig, ax = plot_cm(cm)\n",
        "      output.append((fig, ax))\n",
        "  return output\n",
        "\n",
        "def get_foreground(ary):\n",
        "  \"\"\"\n",
        "  Retreives the foreground from an image \n",
        "\n",
        "  :param ary: A 2D or 3D `numpy.ndarray` of the array(s) to detect the foreground \n",
        "\n",
        "  :returns : A 2D or 3D `numpy.ndarray` of the detected foreground(s)\n",
        "  \"\"\"\n",
        "  if ary.ndim == 3:\n",
        "    return numpy.array([get_foreground(x) for x in ary])\n",
        "  filtered = filters.gaussian(ary, sigma=5)\n",
        "  threshold = filters.threshold_triangle(filtered)\n",
        "  return (filtered >= threshold).astype(int)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lGNVD44lPR0",
        "colab_type": "text"
      },
      "source": [
        "## Load network and infer on testing data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYuLOn1JYvX2",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "import loader \n",
        "from network import UNet\n",
        "\n",
        "minmax = numpy.load(os.path.join(\"raw_data\", \"minmax.npy\"))\n",
        "networks_infos = {\n",
        "    name : {\n",
        "        \"data_path\" : os.path.join(\"raw_data\", f\"data_{name}.npz\"),\n",
        "        \"model_path\" : os.path.join(\"trained-networks\", name)\n",
        "    }\n",
        "    for name in [\"polygonal_bbox\", \"bbox\"]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQPMGsbW9YnT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plots the learning curve of the loaded model\n",
        "for name, network_infos in networks_infos.items():\n",
        "  plot_learning_curves(network_infos[\"model_path\"], title=name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NenW5yywAzXz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selects the desired network from the networks informations and loads it \n",
        "network_infos = networks_infos[\"polygonal_bbox\"]\n",
        "\n",
        "# Loads the data from the cloned folder \n",
        "data = numpy.load(network_infos[\"data_path\"])\n",
        "\n",
        "# Loads the image from data and the indices\n",
        "images, targets = data[\"images\"], data[\"labels\"]\n",
        "train_idx, valid_idx, test_idx = loader.get_idx(data)\n",
        "\n",
        "# Creation of the model\n",
        "model = UNet(in_channels=1, out_channels=2)\n",
        "model.load_model(network_infos[\"model_path\"], cuda=CUDA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyuYtHEkUUVz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Infer the network on the testing dataset\n",
        "for (X, y, pred, idx) in model.predict(images, targets, idx=test_idx, cuda=CUDA, minmax=minmax):\n",
        "  show_random(X, y, pred, samples=5)\n",
        "  pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CP3SFz6ACIBi",
        "colab_type": "text"
      },
      "source": [
        "## Quantitative assessment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6FdFP93oB90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Infer the network on the testing dataset and compute metrics \n",
        "to_compute_metrics = [\"dice\", \"iou\", \"confusion_matrix\", \"precision\", \"recall\", \"accuracy\"]\n",
        "\n",
        "# Predict all images in the testing dataset\n",
        "complete_scores = {\n",
        "    condition : {\n",
        "        name : [] for name in to_compute_metrics\n",
        "    } for condition in [\"bbox\"]\n",
        "}\n",
        "for (X, y, pred, idx) in model.predict(images, targets, idx=test_idx, cuda=CUDA, minmax=minmax):\n",
        "\n",
        "  foregrounds = None\n",
        "  metric_calculator = MetricCalculator(y, pred, foregrounds=foregrounds)\n",
        "  scores = metric_calculator.get(to_compute_metrics)\n",
        "  for metric, score in zip(to_compute_metrics, scores):\n",
        "    complete_scores[\"bbox\"][metric].extend(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYp6y3bxs5qw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose which metrics to show \n",
        "keep_keys = [\"iou\", \"dice\", \"precision\", \"recall\", \"accuracy\"]\n",
        "\n",
        "fig, ax = plot_scores(complete_scores, show_points=True, avail_metrics=keep_keys, ylim=(0, 1))\n",
        "fig, ax = plot_cumfreq(complete_scores, avail_metrics=keep_keys)\n",
        "figs_axes = show_average_cm(complete_scores)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTDvp7aWEeZM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Impact of foreground detection on metrics\n",
        "complete_scores[\"bbox + foreground\"] = {\n",
        "    name : [] for name in to_compute_metrics\n",
        "}\n",
        "for (X, y, pred, idx) in model.predict(images, targets, idx=test_idx, cuda=CUDA, minmax=minmax):\n",
        "\n",
        "  foregrounds = get_foreground(X.squeeze())\n",
        "  metric_calculator = MetricCalculator(y, pred, foregrounds=foregrounds)\n",
        "  scores = metric_calculator.get(to_compute_metrics)\n",
        "  for metric, score in zip(to_compute_metrics, scores):\n",
        "    complete_scores[\"bbox + foreground\"][metric].extend(score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NXYSbWKE9bz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Choose which metrics to show \n",
        "keep_keys = [\"iou\", \"dice\", \"precision\", \"recall\", \"accuracy\"]\n",
        "\n",
        "fig, ax = plot_scores(complete_scores, show_points=True, avail_metrics=keep_keys, ylim=(0, 1))\n",
        "fig, ax = plot_cumfreq(complete_scores, avail_metrics=keep_keys)\n",
        "figs_axes = show_average_cm(complete_scores)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF_7UGCGD0FH",
        "colab_type": "text"
      },
      "source": [
        "# Part 2 : Increased Performance & Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AYjHM-MIvg8",
        "colab_type": "text"
      },
      "source": [
        "## Compare prediction with manual labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wd73Roh3qGNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loads the network model and parameters\n",
        "network_infos = networks_infos[\"polygonal_bbox\"]\n",
        "data = numpy.load(network_infos[\"data_path\"])\n",
        "targets_unprecised = data[\"labels\"]\n",
        "images, targets = data[\"images\"], data[\"labels\"]\n",
        "train_idx, valid_idx, test_idx = loader.get_idx(data)\n",
        "model = UNet(in_channels=1, out_channels=2)\n",
        "model.load_model(network_infos[\"model_path\"], cuda=CUDA)\n",
        "\n",
        "# Predict all images in the testing dataset\n",
        "to_compute_metrics = [\"dice\", \"iou\", \"confusion_matrix\", \"precision\", \"recall\", \"accuracy\"]\n",
        "manual_comparison = {\n",
        "    condition : {\n",
        "        name : [] for name in to_compute_metrics\n",
        "    } for condition in [\"bbox - manual\", \"prediction - manual\", \"prediction - bbox\"]\n",
        "}\n",
        "\n",
        "# Extracts the name of the precise labels\n",
        "_man = glob.glob(os.path.join(\"testing\", \"*_man.tif\"))\n",
        "available_manuals = [int(os.path.basename(man_name).split(\"_\")[0]) for man_name in _man]\n",
        "for (X, y, pred, idx) in model.predict(images, targets, idx=test_idx, cuda=CUDA, minmax=minmax):\n",
        "  keep = [tidx in available_manuals for tidx in test_idx[idx]]\n",
        "  if any(keep):\n",
        "    X, y, pred, idx = X[keep], y[keep], pred[keep], idx[keep]\n",
        "    manual = numpy.stack([io.imread(os.path.join(\"testing\", \"{}_man.tif\".format(test_idx[i]))) for i in idx], axis=0)\n",
        "    \n",
        "    # Shows some examples\n",
        "    show_random(X, y, pred, manual, samples=2)\n",
        "    pyplot.show()\n",
        "\n",
        "    y = y[:, numpy.newaxis, ...]\n",
        "    metric_calculator = MetricCalculator(manual, y)\n",
        "    scores = metric_calculator.get(to_compute_metrics)\n",
        "    for metric, score in zip(to_compute_metrics, scores):\n",
        "      manual_comparison[\"bbox - manual\"][metric].extend(score)\n",
        "\n",
        "    metric_calculator = MetricCalculator(manual, pred)\n",
        "    scores = metric_calculator.get(to_compute_metrics)\n",
        "    for metric, score in zip(to_compute_metrics, scores):\n",
        "      manual_comparison[\"prediction - manual\"][metric].extend(score)\n",
        "\n",
        "    y = y.squeeze()\n",
        "    metric_calculator = MetricCalculator(y, pred)\n",
        "    scores = metric_calculator.get(to_compute_metrics)\n",
        "    for metric, score in zip(to_compute_metrics, scores):\n",
        "      manual_comparison[\"prediction - bbox\"][metric].extend(score)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD5tT-WXLIYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plot_scores(manual_comparison, avail_metrics=[\"iou\"], show_lines=False)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZFWSBZKI59t",
        "colab_type": "text"
      },
      "source": [
        "## Crafting a representative metric\n",
        "\n",
        "The crafted metric for the F-actin dataset on axons uses the spatial frequency information in the extracted masks of an expert and the prediction. \n",
        "\n",
        "The F-actin periodical lattice has a known distance of 190 nm between each rings. This implies that a precise segmentation, for instance a manual segmentation, of the structure should be increased in power frequency. \n",
        "\n",
        "The idea of the metric is that the power frequency of a manual segmentation and precise segmentation should be similar. Hence, there should be a small difference. A very different segmentation of a network should result in a bigger difference.\n",
        "\n",
        "<div>\n",
        "<img src=\"https://drive.google.com/uc?id=1oGHSapjPGtC3bMfJVdYu_jZxevR1zZdy\" width=\"500\"/>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmlsWG5LKIsy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fft_ratio(image, truth, predicted, foreground):\n",
        "  \"\"\"\n",
        "  Computes the Fourier ratio between the pixels in the ground truth mask\n",
        "  and the pixels in the predicted mask\n",
        "\n",
        "  :param image: A 2D `numpy.ndarray` of the image\n",
        "  :param truth: A 2D `numpy.ndarray` of the ground truth mask\n",
        "  :param predicted: A 2D `numpy.ndarray` of the predicted mask\n",
        "  :param foreground: A 2D `numpy.ndarray` of the detected foreground\n",
        "\n",
        "  :returns : The absolute difference of the variation\n",
        "  \"\"\"\n",
        "  truth, predicted, foreground = map(lambda ary : ary.astype(bool), (truth, predicted, foreground))\n",
        "  if (not numpy.any(truth * foreground)) and (not numpy.any(predicted * foreground)):\n",
        "    return 0\n",
        "  ary_1, ary_2 = image * truth * foreground, image * predicted * foreground\n",
        "\n",
        "  fft_orig, freq = fft(image, numpy.ones(image.shape))\n",
        "  fft_truth, freq = fft(ary_1.reshape(image.shape), numpy.ones(image.shape))\n",
        "  fft_pred, freq = fft(ary_2.reshape(image.shape), numpy.ones(image.shape))\n",
        "\n",
        "  # Creates a mesh grid of angles\n",
        "  yy, xx = numpy.meshgrid(*freq)\n",
        "  xx[(xx == 0.)] += 1 # Avoids 0 division\n",
        "  atan = numpy.arctan(yy / xx) * 180 / numpy.pi + 90\n",
        "  atan[xx >= 0] += 180\n",
        "\n",
        "  forig, ftruth, fpred = [], [], []\n",
        "  angles, wavelengths = numpy.arange(0, 360, 10), numpy.arange(170, 200, 10)\n",
        "  for angle in angles:\n",
        "    for wavelength in wavelengths:\n",
        "      z = (xx**2 + yy**2 <= (1 / wavelength)**2) & (xx**2 + yy**2 >= (1 / (wavelength + 10))**2) & \\\n",
        "          (atan >= angle) & (atan <= angle + 10)\n",
        "      forig.append(fft_orig[z].sum())\n",
        "      ftruth.append(fft_truth[z].sum())\n",
        "      fpred.append(fft_pred[z].sum())\n",
        "\n",
        "  forig, ftruth, fpred = map(lambda l : numpy.array(l).reshape(len(angles), len(wavelengths)), (forig, ftruth, fpred))\n",
        "\n",
        "  ratio_truth = (ftruth.sum() - forig.sum()) / fft_orig.sum()\n",
        "  ratio_pred = (fpred.sum() - forig.sum()) / fft_orig.sum()\n",
        "  return abs(ratio_truth - ratio_pred)\n",
        "\n",
        "def fft(ary, foreground, timestamp=20):\n",
        "  \"\"\"\n",
        "  Computes the FFT of an image\n",
        "\n",
        "  :param ary: A 2D `numpy.ndarray`\n",
        "  :param foreground: A 2D `numpy.ndarray` of the detected foreground\n",
        "  \n",
        "  :returns : Fourier transform and the frequency axis\n",
        "  \"\"\"\n",
        "  x = numpy.fft.fft2(ary * foreground)\n",
        "  return numpy.abs(x), tuple(numpy.fft.fftshift(numpy.fft.fftfreq(shape, d=timestamp)) for shape in ary.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9SPoPZ8FuDq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loads the network architecture in memory\n",
        "network_infos = networks_infos[\"polygonal_bbox\"]\n",
        "data = numpy.load(network_infos[\"data_path\"])\n",
        "targets_unprecised = data[\"labels\"]\n",
        "images, targets = data[\"images\"], data[\"labels\"]\n",
        "train_idx, valid_idx, test_idx = loader.get_idx(data)\n",
        "model = UNet(in_channels=1, out_channels=2)\n",
        "model.load_model(network_infos[\"model_path\"], cuda=CUDA)\n",
        "\n",
        "# Predict all images in the testing dataset\n",
        "to_compute_metrics = [\"fft\"]\n",
        "manual_comparison = {\n",
        "    condition : {\n",
        "        name : [] for name in to_compute_metrics\n",
        "    } for condition in [\"prediction - manual\", \"prediction - bbox\"]\n",
        "}\n",
        "\n",
        "_man = glob.glob(os.path.join(\"testing\", \"*_man.tif\"))\n",
        "available_manuals = [int(os.path.basename(man_name).split(\"_\")[0]) for man_name in _man]\n",
        "for (X, y, pred, idx) in model.predict(images, targets, idx=test_idx, cuda=CUDA, minmax=minmax):\n",
        "  keep = [tidx in available_manuals for tidx in test_idx[idx]]\n",
        "  if any(keep):\n",
        "    X, y, pred, idx = X[keep], y[keep], pred[keep], idx[keep]\n",
        "    manual = numpy.stack([io.imread(os.path.join(\"testing\", \"{}_man.tif\".format(test_idx[i]))) for i in idx], axis=0)\n",
        "    \n",
        "    foregrounds = get_foreground(X.squeeze())\n",
        "\n",
        "    show_random(X, y, pred, manual, samples=2)\n",
        "    pyplot.show()\n",
        "\n",
        "    y = y[:, numpy.newaxis, ...]\n",
        "    \n",
        "    for xx, yy, ppred, fforeground in zip(X, y, pred, foregrounds):\n",
        "      ratio = fft_ratio(xx.squeeze(), yy.squeeze(), numpy.argmax(ppred, axis=0), fforeground)\n",
        "      manual_comparison[\"prediction - bbox\"][\"fft\"].append(ratio)\n",
        "\n",
        "    for xx, yy, ppred, fforeground in zip(X, manual, pred, foregrounds):\n",
        "      ratio = fft_ratio(xx.squeeze(), yy.squeeze(), numpy.argmax(ppred, axis=0), fforeground)\n",
        "      manual_comparison[\"prediction - manual\"][\"fft\"].append(ratio)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZ7dpGcTGorq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plot_scores(manual_comparison, avail_metrics=[\"fft\"], ylim=None)\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg3_ECgQ46Q1",
        "colab_type": "text"
      },
      "source": [
        "## Provide a different threshold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4TADPR-NAPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Selects the desired network from the networks informations and loads it \n",
        "network_infos = networks_infos[\"bbox\"]\n",
        "\n",
        "# Loads the data from the cloned folder \n",
        "data = numpy.load(network_infos[\"data_path\"])\n",
        "\n",
        "# Loads the image from data and the indices\n",
        "images, targets = data[\"images\"], data[\"labels\"]\n",
        "train_idx, valid_idx, test_idx = loader.get_idx(data)\n",
        "\n",
        "# Creation of the model\n",
        "model = UNet(in_channels=1, out_channels=2)\n",
        "model.load_model(network_infos[\"model_path\"], cuda=CUDA)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCfOLqRRP-07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Infer the network on the testing dataset and compute metrics \n",
        "to_compute_metrics = [\"precision_recall_curve\"]\n",
        "\n",
        "# Load precise labels from dataset using polygonal_bbox\n",
        "precise_targets = numpy.load(networks_infos[\"polygonal_bbox\"][\"data_path\"])[\"labels\"]\n",
        "\n",
        "# Predict all images in the validation dataset\n",
        "all_scores = {}\n",
        "for (X, y, pred, idx) in model.predict(images, targets, idx=valid_idx, cuda=CUDA, minmax=minmax):\n",
        "\n",
        "  precise = precise_targets[valid_idx[idx]]\n",
        "\n",
        "  foregrounds = get_foreground(X.squeeze())\n",
        "  pred_probas = sigmoid(pred)[:, 1]\n",
        "  pred_probas = pred_probas[:, numpy.newaxis]\n",
        "  metric_calculator = MetricCalculator(precise, pred_probas, foregrounds=foregrounds)\n",
        "  scores = metric_calculator.get(to_compute_metrics)\n",
        "  \n",
        "  for i, score in zip(idx, scores):\n",
        "    all_scores[i] = score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGPXclpdMdqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_threshold(scores, num_valid_images=10, show_dist=False):\n",
        "  \"\"\"\n",
        "  Computes the optimal threshold from the validation images. We simply use the \n",
        "  points from the PR-Curve that is the nearest to a precision of 1 and a recall \n",
        "  of 1.\n",
        "\n",
        "  :param scores: A `dict` where each key is referencing an index in the valid_idx \n",
        "  :param num_valid_image: (Optional) The number of images to keep from valid.\n",
        "                          None results in all images\n",
        "  :param show_dist: (Optional) Wheter to show the distribution of points\n",
        "  \n",
        "  :returns : A `list` of all optimal thresholds \n",
        "  \"\"\"\n",
        "  all_thresholds = []\n",
        "\n",
        "  # Results in all images \n",
        "  if isinstance(num_valid_images, type(None)): \n",
        "    num_valid_images = len(scores) + 1\n",
        "\n",
        "  for image_scores in [values for key, values in scores.items() if key < num_valid_images]:\n",
        "    for metric_score in image_scores:\n",
        "      precision, recall, thresholds = metric_score\n",
        "      pr = numpy.stack([precision, recall]).T\n",
        "      distances = distance.cdist(pr, [[1, 1]])\n",
        "      threshold = thresholds[distances.argmin()]\n",
        "      all_thresholds.append(threshold)\n",
        "\n",
        "  if show_dist:  \n",
        "    fig, ax= pyplot.subplots()\n",
        "    ax.boxplot([all_thresholds])\n",
        "    ax.scatter(numpy.random.normal(loc=1, scale=0.02, size=len(all_thresholds)),\n",
        "              all_thresholds)\n",
        "    ax.set(\n",
        "        ylim=(0, 1), ylabel=\"Threshold\"\n",
        "    )\n",
        "    pyplot.show()\n",
        "\n",
        "  return all_thresholds\n",
        "\n",
        "thresholds = get_threshold(all_scores, num_valid_images=25, show_dist=True)\n",
        "threshold = numpy.median(thresholds)\n",
        "print(threshold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eybsZtOCg13t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load precise labels from dataset \n",
        "precise_targets = numpy.load(networks_infos[\"polygonal_bbox\"][\"data_path\"])[\"labels\"]\n",
        "\n",
        "# Predict all images in the testing dataset\n",
        "to_compute_metrics = [\"dice\", \"iou\", \"confusion_matrix\", \"precision\", \"recall\", \"accuracy\"]\n",
        "naive_threshold_comparison = {\n",
        "    condition : {\n",
        "        name : [] for name in to_compute_metrics\n",
        "    } for condition in [\"prediction - polygonal bbox\", \"prediction - bbox\", \"threshold prediction - polygonal bbox\", \"threshold prediction - bbox\"]\n",
        "}\n",
        "\n",
        "for (X, y, pred, idx) in model.predict(images, targets, idx=test_idx, cuda=CUDA, minmax=minmax):\n",
        "\n",
        "  precise = precise_targets[test_idx[idx]]\n",
        "\n",
        "  foregrounds = get_foreground(X.squeeze())\n",
        "  pred_probas = sigmoid(pred)[:, 1]\n",
        "  pred_probas = pred_probas[:, numpy.newaxis] >= threshold\n",
        "\n",
        "  show_random(X, y, pred, pred_probas, samples=5)\n",
        "\n",
        "  metric_calculator = MetricCalculator(precise, pred_probas, foregrounds=foregrounds)\n",
        "  scores = metric_calculator.get(to_compute_metrics)\n",
        "  for metric, score in zip(to_compute_metrics, scores):\n",
        "      naive_threshold_comparison[\"threshold prediction - polygonal bbox\"][metric].extend(score)\n",
        "\n",
        "  metric_calculator = MetricCalculator(precise, pred, foregrounds=foregrounds)\n",
        "  scores = metric_calculator.get(to_compute_metrics)\n",
        "  for metric, score in zip(to_compute_metrics, scores):\n",
        "      naive_threshold_comparison[\"prediction - polygonal bbox\"][metric].extend(score)    \n",
        "\n",
        "  metric_calculator = MetricCalculator(y, pred, foregrounds=foregrounds)\n",
        "  scores = metric_calculator.get(to_compute_metrics)\n",
        "  for metric, score in zip(to_compute_metrics, scores):\n",
        "      naive_threshold_comparison[\"prediction - bbox\"][metric].extend(score)   \n",
        "\n",
        "  metric_calculator = MetricCalculator(y, pred_probas, foregrounds=foregrounds)\n",
        "  scores = metric_calculator.get(to_compute_metrics)\n",
        "  for metric, score in zip(to_compute_metrics, scores):\n",
        "      naive_threshold_comparison[\"threshold prediction - bbox\"][metric].extend(score)           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Z6k0jNsFZv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plot_scores(naive_threshold_comparison, avail_metrics=[\"precision\"], rotation=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otBn1muCbdVQ",
        "colab_type": "text"
      },
      "source": [
        "## Intersection over union problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q13yCSt3waIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creation of the figure\n",
        "fig, (ax1, ax2, ax3, ax4) = pyplot.subplots(1, 4)\n",
        "\n",
        "# A plain circle centered at (10, 10) with radii of 5 is our ground truth\n",
        "ground_truth = numpy.zeros((20, 20), dtype=numpy.bool)\n",
        "rr, cc = skimage.draw.circle(10, 10, 5)\n",
        "ground_truth[rr, cc] = True\n",
        "ax1.imshow(ground_truth, cmap=\"gray\")\n",
        "ax1.set_title(\"Ground Truth\")\n",
        "\n",
        "# Set a prediction centered at (10, 10) with radii of 4\n",
        "pred_smaller = numpy.zeros((20, 20), dtype=numpy.bool)\n",
        "rr, cc = skimage.draw.circle(10, 10, 4)\n",
        "pred_smaller[rr, cc] = True\n",
        "ax2.imshow(pred_smaller, cmap=\"gray\")\n",
        "dice = MetricCalculator([None], [None])._iou(ground_truth, pred_smaller)\n",
        "ax2.set_title(f\"IOU of {dice:0.2f}\")\n",
        "\n",
        "# Set a prediction centered at (10, 10) with radii of 6\n",
        "pred_smaller = numpy.zeros((20, 20), dtype=numpy.bool)\n",
        "rr, cc = skimage.draw.circle(10, 10, 6)\n",
        "pred_smaller[rr, cc] = True\n",
        "ax3.imshow(pred_smaller, cmap=\"gray\")\n",
        "dice = MetricCalculator([None], [None])._iou(ground_truth, pred_smaller)\n",
        "ax3.set_title(f\"IOU of {dice:0.2f}\")\n",
        "\n",
        "# Set a prediction centered one pixel away from ground truth\n",
        "pred_offset = numpy.zeros((20, 20), dtype=numpy.bool)\n",
        "rr, cc = skimage.draw.circle(10, 11, 5)\n",
        "pred_offset[rr, cc] = True\n",
        "dice = MetricCalculator([None], [None])._iou(ground_truth, pred_offset)\n",
        "ax4.set_title(f\"IOU of {dice:0.2f}\")\n",
        "ax4.imshow(pred_offset, cmap=\"gray\")\n",
        "\n",
        "pyplot.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}